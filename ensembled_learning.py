# -*- coding: utf-8 -*-
"""Ensembled_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qXBSV0XeP6Fk-EIV3RdM5yw93T_wUeOL
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score,classification_report
from sklearn.preprocessing import LabelEncoder
import seaborn as sns

#STACKING

df= sns.load_dataset('iris')

x= df.drop('species',axis=1)
y= df['species']

df.head()

le= LabelEncoder()
y_encoded= le.fit_transform(y)

x_train,x_test,y_train,y_test= train_test_split(x,y_encoded,test_size=0.20,random_state=42,stratify=y_encoded)

base_learners= [('dt',DecisionTreeClassifier(random_state=42)),
                ('svc',SVC(probability=True,kernel='rbf',random_state=42)),
                ('lr',LogisticRegression(max_iter=1000))]
meta_learner=LogisticRegression(max_iter=1000)

stacking_clf= StackingClassifier(
    estimators= base_learners,
    final_estimator= meta_learner,
    cv=5
)

stacking_clf.fit(x_train,y_train)

y_pred_stacking= stacking_clf.predict(x_test)

score= accuracy_score(y_test,y_pred_stacking)

score

#ok now we get accuracy of 96 percent using stacking in Ensemble Learning...

#Bagging....

from sklearn.ensemble import RandomForestClassifier

rf_model= RandomForestClassifier(
    n_estimators=100,   #number of trees
    max_depth=None,
    random_state=42
)

rf_model.fit(x_train,y_train)

y_pred_bagging= rf_model.predict(x_test)

score= accuracy_score(y_pred_bagging,y_test)

score

#Boosting...

from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier

ada_model= AdaBoostClassifier(n_estimators=100,random_state=42)

ada_model.fit(x_train,y_train)

y_pred_ada_boost = ada_model.predict(x_test)

score= accuracy_score(y_pred_ada_boost,y_test)

score

gb_model= GradientBoostingClassifier(n_estimators=100,learning_rate=0.1,
                                     random_state=42)

gb_model.fit(x_train,y_train)

y_pred_gb_boost= gb_model.predict(x_test)

score= accuracy_score(y_test,y_pred_gb_boost)

score

#ada boost and gb boost both are same but the difference is in learning rate....
# ada boost has learning rate 1 and gb boost has learning rate 0.1

#XG boost

from xgboost import XGBClassifier

xgb_model= XGBClassifier(n_estimators=100,learning_rate=0.1,max_depth=3,
                         use_label_encoder=False, eval_metric='mlogloss',random_state=42)

xgb_model.fit(x_train,y_train)

y_pred_xg_boost= xgb_model.predict(x_test)

score = accuracy_score(y_pred_xg_boost,y_test)

score

